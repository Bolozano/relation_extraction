# -*- coding: utf-8 -*-
"""to_py.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NIxPwtDfdIwMeTZ6hFbfNxzSXUrRnXWf

# MaMa Algorithme  -LM to KG
"""

# import torch
# from transformers import BertTokenizer, BertModel
import string
from tqdm.notebook import tqdm
# import pickle
# import gc
import re
import json
# import time
# import numpy as np
# import matplotlib as plt
import sys


# tokenizer = BertTokenizer.from_pretrained('bert-base-cased')  #there are multilingue model
# model = BertModel.from_pretrained('bert-base-cased',output_attentions=True).cuda()

import spacy.cli
spacy.cli.download("en_core_web_sm")

import spacy
nlp=spacy.load('en_core_web_sm')

#@title Neo4J Credentials
neo4jUser = "intern2021" #@param {type:"string"}
from getpass import getpass

neo4jPassword = sys.argv[1]
print (f"neo4jUser: {neo4jUser}")
# print (f"neo4jPassword: {neo4jPassword}")

from py2neo import Graph

graph = Graph("bolt+s://db-3ib8ouj9xqqcy081668k.graphenedb.com:24786",
              auth=(neo4jUser, neo4jPassword))

interesting_verbs=['inves','acqui','join','buy','rais','reach','receiv','parter','Inves','Acqui','Join','Buy','Rais','Reach','Receiv','Parter']
entity_type=['ORG','MONEY']

class entity:

  def __init__(self,start,end,category,name):
    self.start=start
    self.end=end
    self.category=category                                                                                  #for entity token(the lukemodel idea)
    self.name=name                                                                           ##we can try to find if the token is in the tokenize(entity) to judge

class data:

  
  def __init__(self,news):
    self.entities_in_all_phrases=[]
    self.sizes_of_all_phrases=[]
    self.texts=[] 
    self.relations=[]
    self.number_of_phrases=0
    self.relations=[]
    temp_texts=[]
    #from a text we generate a list of phrases     
    for news_item in news:
    #   temp_texts.append([self.delete_paranthese(news_item[0]),news_item[2]])
      temp_texts.append(news_item["title"].replace("– TechCrunch",""))
    temp_texts=[x[0:200] for x in temp_texts]
    for i in range(len(temp_texts)):
      sentence=temp_texts[i]
      doc=nlp(sentence)
      if len(doc.ents)<2:
        continue
      # self.texts.append(temp_texts[i][0])
      # self.add(doc,len(self.texts)-1,temp_texts[i][1])  
      self.texts.append(temp_texts[i])
      self.add(doc,len(self.texts)-1,0)  
    self.number_of_phrases=len(self.texts)

  def add(self,doc,index,number):
    entities=doc.ents
    entities_in_a_phrase=[]
    for item in entities:
      if item.label_ not in entity_type:
        continue
      start=item.start
      end=item.end
      new_entity=entity(start,end,category=item.label_,name=' '.join([i.text for i in doc[start:end]]))
      entities_in_a_phrase.append(new_entity)
    self.entities_in_all_phrases.append(entities_in_a_phrase)
    for i in range(len(entities_in_a_phrase)-1):
      relation=[word for word in doc[entities_in_a_phrase[i].end-1:entities_in_a_phrase[i+1].start]]
      contain,interesting_verb,pos,by=self.contain_interesting_verb(relation)
      if contain==True:
        relation={}
        
        relation['i_th_phrase']=number
        relation['original_txt']=self.get_phrase_i(index)
        
        relation['relation']=' '.join([word.text for word in doc[entities_in_a_phrase[i].end-1:entities_in_a_phrase[i+1].start]])
        if by>pos:
          relation['head']=entities_in_a_phrase[i+1].name
          relation['tail']=entities_in_a_phrase[i].name
        else:
          relation['head']=entities_in_a_phrase[i].name
          relation['tail']=entities_in_a_phrase[i+1].name
        relation['verb']=interesting_verb.text
        relation['root_verb']=interesting_verb.lemma_
        relation['head_type']=entities_in_a_phrase[i].category
        relation['tail_type']=entities_in_a_phrase[i+1].category
        self.relations.append(relation)

        print(relation['original_txt'])
        print(relation['head'],'//',relation['verb'],'//',relation['tail'])
        print()
    
  def contain_interesting_verb(self,relation):
    by=-1
    pos=-1
    for i in range(len(relation)):
      token=relation[i]
      if token.text=='by':
        by=i
        continue
      for verb in interesting_verbs:
        if verb in token.text:
          pos=i
          return True,token,pos,by
    return False,0,pos, by

  def get_entities_in_phrase_i(self,index):
    return self.entities_in_all_phrases[index]
  
  def get_j_entity_in_i_phrase(self,i,j):
    return entities_in_all_phrases[i][j]
  
  def get_number_of_entities_in_phrase_i(self,i):
    return self.sizes_of_all_phrases[i]

  def get_entities_type_in_phrase_i(self,index):
    categories=[]
    for entity in self.entities_in_all_phrases[index]:
      categories.append(entity.category)
    return categories

  def get_entities_name_in_phrase_i(self,index):
    name=[]
    for entity in self.entities_in_all_phrases[index]:
      name.append(entity.name)
    return name

  def delete_paranthese(self,text):
    i=0
    k=0
    while len(text)>i:
      if text[i] != '(':
        if text[i]==')':
          text=text[:i]+text[(i+1):]
          continue
        i=i+1
        continue
      else:
        k=i
        for j in range(i,len(text)):
          if text[j]==')':
            k=j
            break
        text=text[:i]+text[(k+1):]
    return text

  def uncased(self,doc):
    ent_pos=[]
    for entity in doc.ents:
      ent_pos=ent_pos+list(range(entity.start,entity.end))
    sentence=''
    sentence=sentence+doc[0].text
    for i in range(1,len(doc)):
      if i not in ent_pos:
        sentence=sentence+' '
        sentence=sentence+doc[i].lower_  
      else:
        sentence=sentence+' '
        sentence=sentence+doc[i].text
    return sentence
  
  def get_entities_spans_in_phrase_i(self,index):
    spans=[]
    for entity in self.entities_in_all_phrases[index]:
      spans.append((entity.start,entity.end))
    return spans

  def get_phrase_i(self,i):
    return self.texts[i]

  def get_fake_phrase_i(self,i):
    return self.fake_texts[i]

  def get_number_of_phrases(self):
    return len(self.texts)

  def delete_punctuation(self,sentence):
    chars=list(sentence)
    for i in range(len(chars)):
      if chars[i] in string.punctuation and chars[i] not in ['&','%','$','€','£','@']:
        chars[i]=' '
    return ''.join(chars)

"""# Matching"""

# def to_gpu(Tokenizer_output):
#   tokens_tensor = Tokenizer_output['input_ids'][:,:500].cuda()
#   token_type_ids = Tokenizer_output['token_type_ids'][:,:500].cuda()
#   attention_mask = Tokenizer_output['attention_mask'][:,:500].cuda()

#   output = {'input_ids' : tokens_tensor, 
#             'token_type_ids' : token_type_ids, 
#             'attention_mask' : attention_mask}

#   return output

"""# TREAT"""

relations=[]

def treat(relations,news):
  # texts=input_data.texts
  k=0
  web_data=data(news)
  relations=relations+web_data.relations
  h = open("simpleacquire.json.txt", 'w+')
  new_json=json.dumps(relations,indent=1)
  h.write(new_json)
  h.close()
  return relations

news = graph.query("""
    MATCH (n:News) 
    WHERE 
        n.publicationDate >= $before
    AND n.publicationDate < $after
    AND size(n.title) > 10
  RETURN n.title as title        
  """, {"before": sys.argv[2], "after": sys.argv[3]})

treat(relations,news)
